<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<title>Iksung KANG</title>
	<meta name="author" content="Iksung KANG">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
	<table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		<tr style="padding:0px">
			<td style="padding:0px">
				<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
					<tr style="padding:0px">
						<td style="padding:2.5%;width:63%;vertical-align:middle">
							<p style="text-align:center">
								<name>Iksung KANG</name>
								<br>
								<email>{first name}.{last name} at berkeley.edu</email>
							</p>
							<p>
								I am currently working as a postdoctoral fellow at UC Berkeley, advised by <a href="https://www.jilab.net/">Professor Na Ji</a> and <a href="https://web.eecs.umich.edu/~stellayu/">Stella X. Yu</a>, and a recent PhD from <a href="https://www.eecs.mit.edu/">MIT EECS</a>.
							</p>
							<p>
								At MIT, I was advised by <a href="http://optics.mit.edu/">Professor George Barbastathis</a> and was a recipient of <a href="https://sch.kfas.or.kr/">Korea Foundation for Advanced Studies</a> scholarship throughout my doctoral study.
							</p>
							<p>
								<strong>I am currently on the 2024-25 academic job market.</strong>
							</p>
							<p style="text-align:center">
								<a href="mailto:iksung.kang@berkeley.edu">Email</a> &nbsp/&nbsp
								<a href="data/Curriculum_Vitae__Iksung_Kang.pdf">CV</a> &nbsp/&nbsp
<!-- 								<a href="https://scholar.google.com/citations?user=WbDsUMcAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp -->
								<a href="https://github.com/iksungk/">Github</a>
							</p>
						</td>
						<td style="padding:2.5%;width:40%;max-width:40%">
							<a href="images/Photo_IK.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Photo_IK_circle.png" class="hoverZoomLink"></a>
						</td>
					</tr>
					</tbody>
				</table>


<!-- 				<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
					<tr>
						<td style="padding-left:20px;padding-right:20px;width:100%;vertical-align:middle">
							<heading>News</heading>
							<td style="padding-left:20px;padding-right:20px;padding-top:5px;padding-bottom:5px;width:100%;vertical-align:middle">
								
							</td>
						</td>
					</tr>
					</tbody>
				</table> -->

				
				<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
					<tr>
						<td style="padding-left:20px;padding-right:20px;width:100%;vertical-align:middle">
							<heading>Research</heading>
							<p>
								I am enthusiastic about applying <em>machine learning to biomedicine through optical imaging</em>. Collaboration and impact have been central to my research career. I have consistently worked closely with diverse teams of professionals to address important challenges in the field. Additionally, I prioritize ensuring that the developed methods and tools are accessible to general users. By upholding these values, I aim to contribute to developing and democratizing both hardware- and software-based tools for imaging and conducting high-impact research in biomedicine.
							</p>
						</td>
					</tr>
					</tbody>
				</table>
				
				<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
					<tr>
						<td style="padding-left:20px;padding-right:20px;width:100%;vertical-align:middle">
							<h3>Preprints</h3>
						</td>
					</tr>

					<tr>
<!-- 						<td style="padding:20px;width:25%;vertical-align:middle">
							<img src="images/demosaic_intro.png" width="160">
						</td> -->
						<td style="padding-left:20px;padding-right:20px;padding-top:5px;padding-bottom:5px;width:100%;vertical-align:middle">
							<a href="https://www.biorxiv.org/content/10.1101/2023.11.10.566599v1.abstract">
							<papertitle>Optical segmentation-based compressed readout of neuronal voltage dynamics</papertitle>
							</a>
							<br>
							Kim S, Ko G, <strong>Kang I</strong>, Tian H, Fan LZ, Li Y, Cohen AE, Wu J, Dai Q, Choi MM
							<br>
							<em>bioRxiv</em> (2023) 2023.11.10.566599. <a href="https://doi.org/10.1101/2023.11.10.566599">https://doi.org/10.1101/2023.11.10.566599</a>
						</td>
					</tr>	
					
					<tr>
						<td style="padding-left:20px;padding-right:20px;width:100%;vertical-align:middle">
							<h3>Journal Publications</h3>
						</td>
					</tr>
				
					<tr>
<!-- 						<td style="padding:20px;width:25%;vertical-align:middle">
							<img src="images/clean_promo.jpg" alt="clean-usnob" width="160" height="160">
						</td> -->
						<td style="padding-left:20px;padding-right:20px;padding-top:5px;padding-bottom:5px;width:100%;vertical-align:middle">
							<a href="https://doi.org/10.1038/s42256-024-00853-3">
							<papertitle>Coordinate-based neural representations for computational adaptive optics in widefield microscopy</papertitle>
							</a>
							<br>
							<strong>Kang I*</strong>, Zhang Q*, Yu SX, Ji N
							<br>
							<em>Nature Machine Intelligence</em> (2024) 6, 714-725. <a href="https://doi.org/10.1038/s42256-024-00853-3">https://doi.org/10.1038/s42256-024-00853-3</a>
							<br>
							*Contributed equally and co-correspondence authors.
						</td>
					</tr>
					
					<tr>
<!-- 						<td style="padding:20px;width:25%;vertical-align:middle">
							<img src="images/clean_promo.jpg" alt="clean-usnob" width="160" height="160">
						</td> -->
<!-- 						<td width="75%" valign="middle"> -->
						<td style="padding-left:20px;padding-right:20px;padding-top:5px;padding-bottom:5px;width:100%;vertical-align:middle">
							<a href="https://doi.org/10.1364/OPTICA.492666">
							<papertitle>Accelerated deep self-supervised ptycho-laminography for three-dimensional nanoscale imaging of integrated circuits</papertitle>
							</a>
							<br>
							<strong>Kang I</strong>, Jiang Y, Holler M, Guizar-Sicairos M, Levi AFJ, Klug J, Vogt S, Barbastathis G
							<br>
							<em>Optica</em> (2023) 10(8), 1000-1008. <a href="https://doi.org/10.1364/OPTICA.492666">https://doi.org/10.1364/OPTICA.492666</a>
						</td>
					</tr>
					
					<tr>
<!-- 						<td style="padding:20px;width:25%;vertical-align:middle">
							<img src="images/clean_promo.jpg" alt="clean-usnob" width="160" height="160">
						</td> -->
<!-- 						<td width="75%" valign="middle"> -->
						<td style="padding-left:20px;padding-right:20px;padding-top:5px;padding-bottom:5px;width:100%;vertical-align:middle">
							<a href="https://www.nature.com/articles/s41377-023-01181-8">
							<papertitle>Attentional Ptycho-Tomography (APT) for three-dimensional nanoscale X-ray imaging with minimal data acquisition and computation time</papertitle>
							</a>
							<br>
							<strong>Kang I*</strong>, Wu Z*, Jiang Y, Yao Y, Deng J, Klug J, Vogt S, Barbastathis G
							<br>
							<em>Light: Science & Applications</em> (2023) 12(131). <a href="https://www.nature.com/articles/s41377-023-01181-8">https://www.nature.com/articles/s41377-023-01181-8</a>
							<br>
							News: <a href="https://www.eurekalert.org/news-releases/991113">EurekAlert!</a>
							<br>
							*Contributed equally.
						</td>
					</tr>
					
					<tr>
<!-- 						<td style="padding:20px;width:25%;vertical-align:middle">
							<img src="images/clean_promo.jpg" alt="clean-usnob" width="160" height="160">
						</td> -->
<!-- 						<td width="75%" valign="middle"> -->
						<td style="padding-left:20px;padding-right:20px;padding-top:5px;padding-bottom:5px;width:100%;vertical-align:middle">
							<a href="https://link.springer.com/article/10.1186/s43593-022-00037-9">
							<papertitle>Three-dimensional nanoscale reduced-angle ptycho-tomographic imaging with deep learning (RAPID)</papertitle>
							</a>
							<br>
							Wu Z*, <strong>Kang I*</strong>, Yao Y, Jiang Y, Deng J, Klug J, Vogt S, Barbastathis G
							<br>
							<em>eLight</em> (2023) 3(7). <a href="https://doi.org/10.1186/s43593-022-00037-9">https://doi.org/10.1186/s43593-022-00037-9</a>
							<br>
							*Contributed equally.
						</td>
					</tr>
					
					<tr>
<!-- 						<td style="padding:20px;width:25%;vertical-align:middle">
							<img src="images/clean_promo.jpg" alt="clean-usnob" width="160" height="160">
						</td> -->
<!-- 						<td width="75%" valign="middle"> -->
						<td style="padding-left:20px;padding-right:20px;padding-top:5px;padding-bottom:5px;width:100%;vertical-align:middle">
							<a href="https://doi.org/10.1364/OPTICA.470712">
							<papertitle>Simultaneous spectral recovery and CMOS micro-LED holography with an untrained deep neural network</papertitle>
							</a>
							<br>
							<strong>Kang I*</strong>, de Cea M*, Xue J, Li Z, Barbastathis G, Ram R
							<br>
							<em>Optica</em> (2022) 9(10), 1149-1155. <a href="https://doi.org/10.1364/OPTICA.470712">https://doi.org/10.1364/OPTICA.470712</a>
							<br>
							*Contributed equally.
						</td>
					</tr>
					
					<tr>
<!-- 						<td style="padding:20px;width:25%;vertical-align:middle">
							<img src="images/clean_promo.jpg" alt="clean-usnob" width="160" height="160">
						</td> -->
<!-- 						<td width="75%" valign="middle"> -->
						<td style="padding-left:20px;padding-right:20px;padding-top:5px;padding-bottom:5px;width:100%;vertical-align:middle">
							<a href="https://doi.org/10.1038/s41377-021-00512-x">
							<papertitle>Dynamical machine learning volumetric reconstruction of objects‚Äô interiors from limited angular views</papertitle>
							</a>
							<br>
							<strong>Kang I*</strong>, <a href="https://scholar.google.com/citations?user=cza7nLcAAAAJ&hl=en">Goy A</a>, <a href="https://scholar.google.com/citations?hl=en&user=4rsJDwUAAAAJ">Barbastathis G</a>
							<br>
							<em>Light: Science & Applications</em> (2021) 10(74). <a href="https://doi.org/10.1038/s41377-021-00512-x">https://doi.org/10.1038/s41377-021-00512-x</a>
							<br>
							News: <a href="https://www.eurekalert.org/news-releases/777818">EurekAlert!</a>
							<br>
							*Correspondence author.
						</td>
					</tr>
					
					<tr>
						<td style="padding-left:20px;padding-right:20px;padding-top:5px;padding-bottom:5px;width:100%;vertical-align:middle">
							<a href="https://doi.org/10.1364/OE.412890">
							<papertitle>Recurrent neural network reveals transparent objects through scattering media</papertitle>
							</a>
							<br>
							<strong>Kang I*</strong>, Pang S, Zhang Q, Fang N, <a href="https://scholar.google.com/citations?hl=en&user=4rsJDwUAAAAJ">Barbastathis G</a>
							<br>
							<em>Optics Express</em> (2020) 29(4), 5316-5326. <a href="https://doi.org/10.1364/OE.412890">https://doi.org/10.1364/OE.412890</a>
							<br>
							*Correspondence author.
						</td>
					</tr>
					
					<tr>
						<td style="padding-left:20px;padding-right:20px;padding-top:5px;padding-bottom:5px;width:100%;vertical-align:middle">
							<a href="https://doi.org/10.1364/OE.397790">
							<papertitle>Deep residual learning for low-order wavefront sensing in high-contrast imaging systems</papertitle>
							</a>
							<br>
							Allan G*, <strong>Kang I*</strong>, Douglas E, <a href="https://scholar.google.com/citations?hl=en&user=4rsJDwUAAAAJ">Barbastathis G</a>, Cahoy K
							<br>
							<em>Optics Express</em> (2020) 28(18), 26267-26283 (2020). <a href="https://doi.org/10.1364/OE.397790">https://doi.org/10.1364/OE.397790</a>
							<br>
							*Contributed equally.
						</td>
					</tr>
					
					<tr>
						<td style="padding-left:20px;padding-right:20px;padding-top:5px;padding-bottom:5px;width:100%;vertical-align:middle">
							<a href="https://doi.org/10.1364/OE.395204">
							<papertitle>On the interplay between physical and content priors in deep learning for computational imaging</papertitle>
							</a>
							<br>
							Deng M, Li S, Zhang Z, <strong>Kang I</strong>, Fang N, <a href="https://scholar.google.com/citations?hl=en&user=4rsJDwUAAAAJ">Barbastathis G</a>
							<br>
							<em>Optics Express</em> (2020) 28(16), 24152-24170. <a href="https://doi.org/10.1364/OE.395204">https://doi.org/10.1364/OE.395204</a>
						</td>
					</tr>
					
					<tr>
						<td style="padding-left:20px;padding-right:20px;padding-top:5px;padding-bottom:5px;width:100%;vertical-align:middle">
							<a href="https://doi.org/10.1364/OE.397430">
							<papertitle>Phase Extraction Neural Network (PhENN) with Coherent Modulation Imaging (CMI) for phase retrieval at low photon counts</papertitle>
							</a>
							<br>
							<strong>Kang I*</strong>, Zhang F, <a href="https://scholar.google.com/citations?hl=en&user=4rsJDwUAAAAJ">Barbastathis G</a>
							<br>
							<em>Optics Express</em> (2020) 28(15), 21578-21600. <a href="https://doi.org/10.1364/OE.397430">https://doi.org/10.1364/OE.397430</a>
							<br>
							*Correspondence author.
						</td>
					</tr>
					
					<tr>
						<td style="padding-left:20px;padding-right:20px;padding-top:5px;padding-bottom:5px;width:100%;vertical-align:middle">
							<a href="https://doi.org/10.1038/s41377-020-0267-2">
							<papertitle>Learning to synthesize: Robust phase retrieval at low photon counts</papertitle>
							</a>
							<br>
							Deng M, Li S, Goy A, <strong>Kang I</strong>, <a href="https://scholar.google.com/citations?hl=en&user=4rsJDwUAAAAJ">Barbastathis G</a>
							<br>
							<em>Light: Science & Applications</em>, 9(36) (2020). <a href="https://doi.org/10.1038/s41377-020-0267-2">https://doi.org/10.1038/s41377-020-0267-2</a>
						</td>
					</tr>
					
					<tr>
						<td style="padding-left:20px;padding-right:20px;padding-top:5px;width:100%;vertical-align:middle">
							<h3>Conference Proceedings & Presentations</h3>
						</td>
					</tr>

					<tr>
						<td style="padding-left:20px;padding-right:20px;padding-top:5px;padding-bottom:5px;width:100%;vertical-align:middle">
							<a href="https://doi.org/10.1117/12.3008468">
							<papertitle>Coordinate-based neural representations for computational adaptive optics in two-photon microscopy</papertitle>
							</a>
							<br>
							<strong>Kang I*</strong>, Zhang Q, Yaeger C, Pham T, Yu SX, Harnett M, Ji N
							<br>
							<em>SPIE Photonics West</em> (2024) 12851-9 (28 January 2024). <a href="https://doi.org/10.1117/12.3008468">https://doi.org/10.1117/12.3008468</a>
							<br>
							*Speaker, oral presentation.
						</td>
					</tr>

					<tr>
						<td style="padding-left:20px;padding-right:20px;padding-top:5px;padding-bottom:5px;width:100%;vertical-align:middle">
							<a href="https://doi.org/10.1117/12.2655261">
							<papertitle>On the use of deep learning for three-dimensional computational imaging</papertitle>
							</a>
							<br>
							Barbastathis G, Pang S, <strong>Kang I*</strong>, Wu Z, Liu Z, Guo Z, Zhang F
							<br>
							<em>SPIE Photonics West</em> (2023) 124450J (8 March 2023). <a href="https://doi.org/10.1117/12.2655261">https://doi.org/10.1117/12.2655261</a>
							<br>
							*Speaker, oral presentation.
						</td>
					</tr>
					
					<tr>
						<td style="padding-left:20px;padding-right:20px;padding-top:5px;padding-bottom:5px;width:100%;vertical-align:middle">
							<a href="https://doi.org/10.1117/12.2658934">
							<papertitle>Deep self-supervised learning for computational adaptive optics in widefield microscopy</papertitle>
							</a>
							<br>
							<strong>Kang I*</strong>, Zhang Q, Ji N
							<br>
							<em>SPIE Photonics West</em> (2023) 123880H (16 March 2023). <a href="https://doi.org/10.1117/12.2658934">https://doi.org/10.1117/12.2658934</a>
							<br>
							*Speaker, oral presentation.
						</td>
					</tr>


					<tr>
						<td style="padding-left:20px;padding-right:20px;padding-top:5px;padding-bottom:5px;width:100%;vertical-align:middle">
							<papertitle>Optical segmentation for compressed readout on sub-millisecond neuronal circuit dynamics ‚Äì Diffractive Multisite Optical Segmentation Assisted Image Compression: DeMOSAIC</papertitle>
							</a>
							<br>
							Kim S, Wu J, <strong>Kang I</strong>, Ko G, Tian H, Fan LZ, Li Y, Cohen AE, Dai Q, Choi MM
							<br>
							<em>Frontiers in Neurophotonics (FiNs)</em> (2022).
						</td>
					</tr>


					<tr>
						<td style="padding-left:20px;padding-right:20px;padding-top:5px;padding-bottom:5px;width:100%;vertical-align:middle">
							<a href="https://doi.org/10.1364/COSI.2022.CF1D.6">
							<papertitle>Photon-starved X-ray Ptychographic Imaging using Spatial Pyramid Atrous Convolution End-to-end Reconstruction (PtychoSPACER)</papertitle>
							</a>
							<br>
							Wu Z, <strong>Kang I</strong>, Zhou T, Coykendall V, Ge B, Cherukara MJ, Barbastathis G
							<br>
							<em>Computational Optical Sensing and Imaging (COSI)</em> (2022) CF1D.6. <a href="https://doi.org/10.1364/COSI.2022.CF1D.6">https://doi.org/10.1364/COSI.2022.CF1D.6</a>
						</td>
					</tr>

					<tr>
						<td style="padding-left:20px;padding-right:20px;padding-top:5px;padding-bottom:5px;width:100%;vertical-align:middle">
							<papertitle>Adaptive image segmentation for crosstalk-free high-speed compressive imaging</papertitle>
							</a>
							<br>
							Kim S, Wu J, <strong>Kang I</strong>, Li Y, Tian H, Fan LZ, Cohen AE, Dai Q, Choi MM
							<br>
							<em>Focus on Microscopy (FOM)</em> (2022).
						</td>
					</tr>
					
					<tr>
						<td style="padding-left:20px;padding-right:20px;padding-top:5px;padding-bottom:5px;width:100%;vertical-align:middle">
							<a href="https://doi.org/10.1364/COSI.2021.CTu6A.4">
							<papertitle>Three-dimensional reconstruction of integrated circuits by single-angle X-ray ptychography with machine learning</papertitle>
							</a>
							<br>
							<strong>Kang I*</strong>, Yao Y, Deng J, Klug J, Vogt S, Honig S, Barbastathis G
							<br>
							<em>Computational Optical Sensing and Imaging (COSI)</em> (2021) CTu6A.4. <a href="https://doi.org/10.1364/COSI.2021.CTu6A.4">https://doi.org/10.1364/COSI.2021.CTu6A.4</a>
							<br>
							*Speaker, oral presentation.
						</td>
					</tr>
					
					<tr>
						<td style="padding-left:20px;padding-right:20px;padding-top:5px;padding-bottom:5px;width:100%;vertical-align:middle">
							<a href="https://doi.org/10.1117/12.2577264">
							<papertitle>Probability of error as an image metric for the assessment of tomographic reconstruction of dense-layered binary-phase objects</papertitle>
							</a>
							<br>
							<strong>Kang I*</strong>, <a href="https://scholar.google.com/citations?hl=en&user=4rsJDwUAAAAJ">Barbastathis G</a>
							<br>
							<em>SPIE Photonics West</em> (2021) 116530T (5 March 2021). <a href="https://doi.org/10.1117/12.2577264">https://doi.org/10.1117/12.2577264</a>
							<br>
							*Speaker, oral presentation.
						</td>
					</tr>
					
					<tr>
						<td style="padding-left:20px;padding-right:20px;padding-top:5px;padding-bottom:5px;width:100%;vertical-align:middle">
							<a href="https://doi.org/10.1117/12.2562927">
							<papertitle>Deep neural networks to improve the dynamic range of Zernike phase-contrast wavefront sensing in high-contrast imaging systems</papertitle>
							</a>
							<br>
							Allan G, <strong>Kang I</strong>, Douglas E, N'Diaye M, <a href="https://scholar.google.com/citations?hl=en&user=4rsJDwUAAAAJ">Barbastathis G</a>, Cahoy K
							<br>
							<em>SPIE Astronomical Telescopes + Instrumentation</em> (2020) 1144349 (13 December 2020). <a href="https://doi.org/10.1117/12.2562927">https://doi.org/10.1117/12.2562927</a>
						</td>
					</tr>
					
					<tr>
						<td style="padding-left:20px;padding-right:20px;padding-top:5px;padding-bottom:5px;width:100%;vertical-align:middle">
							<a href="https://doi.org/10.1109/EMBC.2017.8037619">
							<papertitle>A portable, low-cost, 3D-printed main magnetic field system for magnetic imaging</papertitle>
							</a>
							<br>
							<strong>Kang I*</strong>
							<br>
							<em>IEEE Engineering in Medicine and Biology Society (EMBS)</em> (2017). <a href="https://doi.org/10.1109/EMBC.2017.8037619">https://doi.org/10.1109/EMBC.2017.8037619</a>
							<br>
							*Speaker, oral presentation.
						</td>
					</tr>
					
<!-- 					<tr>
						<td style="padding-left:20px;padding-right:20px;padding-top:5px;width:100%;vertical-align:middle">
							<h3>Technical Reports</h3>
						</td>
					</tr>
					
					<tr>
						<td style="padding-left:20px;padding-right:20px;padding-top:5px;padding-bottom:5px;width:100%;vertical-align:middle">
							<a href="https://www.mtl.mit.edu/sites/default/files/uploads/photonics_and_optoelectronics.pdf#page=16">
							<papertitle>LION: Learning to Invert 3D Objects by Neural Networks</papertitle>
							</a>
							<br>
							<a href="https://scholar.google.com/citations?hl=en&user=4rsJDwUAAAAJ">George Barbastathis</a>, Jungki Song, Ziling Wu, <strong>Iksung Kang</strong>, Subeen Pang, Zhen Guo
							<br>
							<em>Microsystems Annual Research Report</em> (2021)
						</td>
					</tr>
					
					<tr>
						<td style="padding-left:20px;padding-right:20px;padding-top:5px;padding-bottom:5px;width:100%;vertical-align:middle">
							<a href="https://www.mtl.mit.edu/sites/default/files/uploads/photonics_and_optoelectronics.pdf#page=6">
							<papertitle>Imaging Transparent Objects through Dynamic Scattering Media Using Recurrent Neural Networks</papertitle>
							</a>
							<br>
							<strong>Iksung Kang</strong>, Subeen Pang, Qihang Zhang, Nicholas Fang, <a href="https://scholar.google.com/citations?hl=en&user=4rsJDwUAAAAJ">George Barbastathis</a>
							<br>
							<em>Microsystems Annual Research Report</em> (2021)
						</td>
					</tr>
					
					<tr>
						<td style="padding-left:20px;padding-right:20px;padding-top:5px;padding-bottom:5px;width:100%;vertical-align:middle">
							<a href="https://www.mtl.mit.edu/sites/default/files/uploads/section_05.pdf#page=8">
							<papertitle>On the Use of Deep Learning for Retrieving Phase from Noisy Inputs in the Coherent Modulation Imaging Scheme</papertitle>
							</a>
							<br>
							<strong>Iksung Kang</strong>, Fucai Zhang, <a href="https://scholar.google.com/citations?hl=en&user=4rsJDwUAAAAJ">George Barbastathis</a>
							<br>
							<em>Microsystems Annual Research Report</em> (2020)
						</td>
					</tr> -->
					
					</tbody>
				</table>
				
				<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
					<tr>
						<td style="padding-left:20px;padding-right:20px;padding-top:30px;width:100%;vertical-align:middle">
							<heading>Teaching</heading>
							<p>
								Each classroom should foster an inclusive learning environment that embraces and respects the diverse backgrounds of students, while also providing equitable opportunities for learning and practice.
							</p>
						</td>
					</tr>
					
					<tr>
						<td style="padding-left:20px;padding-right:20px;padding-top:8px;padding-bottom:8px;width:100%;vertical-align:middle">
							<strong><a href="https://tll.mit.edu/programming/grad-student-programming/kaufman-teaching-certificate-program/">Kaufman Teaching Certificate Program (KTCP)</a></strong>, Teaching & Learning Laboratory, Massachusetts Institute of Technology (2021)
							<br>
							- <em>Workshop</em>: I completed seven workshops to develop teaching skills as part of the teaching certificate program. A major part of the program involved introducing students to relevant research in teaching and learning and laying out future teaching models.
							<br>
							- <em>Microteaching sessions</em>: I also presented two microteaching sessions that were videotaped, where I received feedback on my performance regarding my teaching and provided feedback to other participants.
						</td>
					</tr>
					
					<tr>
						<td style="padding-left:20px;padding-right:20px;padding-top:8px;padding-bottom:8px;width:100%;vertical-align:middle">
							<strong>Teaching Assistant (<a href="http://student.mit.edu/catalog/m2a.html">2.16/2.168 Learning Machines</a>)</strong>, Department of Mechanical Engieering, Massachusetts Institute of Technology (2020)
							<br>
							I mentored course research projects, contributed to curriculum design, conducted after-hour office hours, and graded assignments. Class taught totaled around 40 students and comprised course research projects on the connection between machine learning and physical systems.
						</td>
					</tr>
						
					</tbody>
				</table>
	
				<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
					<tr>
						<td style="padding-left:20px;padding-right:20px;padding-top:30px;width:100%;vertical-align:middle">
							<heading>Mentoring</heading>
							<p>
								One's education becomes truly meaningful when they share their experiences with others who can benefit from them.
							</p>
						</td>
					</tr>
					
					<tr>
						<td style="padding-left:20px;padding-right:20px;padding-top:8px;padding-bottom:8px;width:100%;vertical-align:middle">
							<strong>Course Project Mentor</strong>, Massachusetts Institute of Technology (2022)
							<br>
							- <a href="https://meche.mit.edu/featured-classes/physical-systems-modeling-and-design-using-machine-learning"><em>2.C01/2.C51 Physical Systems Modeling and Design Using Machine Learning</em></a>: I mentored a graduate student group for their end-term project on the image segmentation of noisy ultrasonic images.
							<br>
							- <em>Mentored students</em>: Anlage AM, Huang Y, Fayer I.
						</td>
					</tr>
					
					<tr>
						<td style="padding-left:20px;padding-right:20px;padding-top:8px;padding-bottom:8px;width:100%;vertical-align:middle">
							<strong>Course Project Mentor</strong>, Massachusetts Institute of Technology (2020)
							<br>
							- <a href="http://student.mit.edu/catalog/m2a.html"><em>2.16/2.168 Learning Machines</em></a>: I mentored two graduate student groups for their end-term projects on (1) the reaction modeling to facilitate pharmaceutical process development using machine learning; and (2) the control of autonomous ocean vehicles using reinforcement learning.
							<br>
							- <em>Mentored students</em>: (1) Eyke NS, Russell BD, Lee RW-Y; and (2) Fountain TS, McGee WA, Cho HS, Edskes BK.
						</td>
					</tr>
					
					<tr>
						<td style="padding-left:20px;padding-right:20px;padding-top:8px;padding-bottom:8px;width:100%;vertical-align:middle">
							<strong><a href="https://friendream.or.kr/76/?q=YToyOntzOjEyOiJrZXl3b3JkX3R5cGUiO3M6MzoiYWxsIjtzOjQ6InBhZ2UiO2k6NTt9&bmode=view&idx=3841743&t=board">Volunteer</a></strong>, Korea Foundation for Advanced Studies Overseas Program (2018)
							<br>
							I participated as a volunteer in the Kingdom of Cambodia for a week, teaching children physics and building houses. 						
						</td>
					</tr>
					
					</tbody>
				</table>
		
<!--           <tr onmouseout="malle_stop()" onmouseover="malle_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <img src='images/MalleConv_after.jpg' width="160"></div>
                <img src='images/MalleConv_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('malle_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('malle_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://yifanjiang.net/MalleConv.html">
                <papertitle>Fast and High-quality Image Denoising via Malleable Convolutions</papertitle>
              </a>
              <br>
              <a href="https://yifanjiang.net/">Yifan Jiang</a>,
              <a href="https://bartwronski.com/">Bartlomiej Wronski</a>, 
							<a href="https://bmild.github.io/">Ben Mildenhall</a>, <br>
              <strong>Jonathan T. Barron</strong>,
              <a href="https://spark.adobe.com/page/CAdrFMJ9QeI2y/">Zhangyang Wang</a>,
              <a href="https://people.csail.mit.edu/tfxue/">Tianfan Xue</a>
              <br>
              <em>ECCV</em>, 2022
              <br>
              <a href="https://yifanjiang.net/MalleConv.html">project page</a>
              /
              <a href="https://arxiv.org/abs/2201.00392">arXiv</a>
              <p></p>
              <p>
              We denoise images efficiently by predicting spatially-varying kernels at low resolution and using a fast fused op to jointly upsample and apply these kernels at full resolution.
              </p>
            </td>
          </tr>
					
          <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nerfsuper_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/nerf_supervision.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/nerf_supervision.jpg' width="160">
              </div>
              <script type="text/javascript">
                function nerfsuper_start() {
                  document.getElementById('nerfsuper_image').style.opacity = "1";
                }

                function nerfsuper_stop() {
                  document.getElementById('nerfsuper_image').style.opacity = "0";
                }
                nerfsuper_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://waymo.com/research/block-nerf/">
                <papertitle>NeRF-Supervision: Learning Dense Object Descriptors from Neural Radiance Fields</papertitle>
              </a>
              <br>
              <a href="https://yenchenlin.me/">Lin Yen-Chen</a>, 
              <a href="http://www.peteflorence.com/">Pete Florence</a>, 
              <strong>Jonathan T. Barron</strong>,  <br>
              <a href="https://scholar.google.com/citations?user=_BPdgV0AAAAJ&hl=en">Tsung-Yi Lin</a>, 
              <a href="https://meche.mit.edu/people/faculty/ALBERTOR@MIT.EDU">Alberto Rodriguez</a>,
              <a href="http://web.mit.edu/phillipi/">Phillip Isola</a>
              <br>
              <em>ICRA</em>, 2022  
              <br>
		<a href="http://yenchenlin.me/nerf-supervision/">project page</a> / 
		<a href="https://arxiv.org/abs/2203.01913">arXiv</a> / 
		<a href="https://www.youtube.com/watch?v=_zN-wVwPH1s">video</a> /
		<a href="https://github.com/yenchenlin/nerf-supervision-public">code</a> / 
		<a href="https://colab.research.google.com/drive/13ISri5KD2XeEtsFs25hmZtKhxoDywB5y?usp=sharing">colab</a>				
              <p></p>
              <p>NeRF works better than RGB-D cameras or multi-view stereo when learning object descriptors.</p>
            </td>
          </tr>
						
          <tr onmouseout="mip360_stop()" onmouseover="mip360_start()"  bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mip360_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/mip360_sat.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/mip360_sat.jpg' width="160">
              </div>
              <script type="text/javascript">
                function mip360_start() {
                  document.getElementById('mip360_image').style.opacity = "1";
                }

                function mip360_stop() {
                  document.getElementById('mip360_image').style.opacity = "0";
                }
                mip360_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://jonbarron.info/mipnerf360">
                <papertitle>Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields</papertitle>
              </a>
              <br>
              <strong>Jonathan T. Barron</strong>,
              <a href="https://bmild.github.io/">Ben Mildenhall</a>,
              <a href="https://scholar.harvard.edu/dorverbin/home">Dor Verbin</a>,
              <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
              <a href="https://phogzone.com/">Peter Hedman</a>
              <br>
							<em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="http://jonbarron.info/mipnerf360">project page</a>
              /
              <a href="https://arxiv.org/abs/2111.12077">arXiv</a>
              /
              <a href="https://youtu.be/zBSH-k9GbV4">video</a>
              <p></p>
              <p>mip-NeRF can be extended to produce realistic results on unbounded scenes.</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/clean_promo.jpg" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://drive.google.com/file/d/1YvRx-4hrZoCk-nl6OgVJZlHAqOiN5hWq/view?usp=sharing">
                <papertitle>Cleaning the USNO-B Catalog Through Automatic Detection of Optical Artifacts</papertitle>
              </a>
              <br>
              <strong>Jonathan T. Barron</strong>, <a href="http://stumm.ca/">Christopher Stumm</a>, <a href="http://cosmo.nyu.edu/hogg/">David W. Hogg</a>, <a href="http://www.astro.princeton.edu/~dstn/">Dustin Lang</a>, <a href="http://cs.nyu.edu/~roweis/">Sam Roweis</a>
              <br>
              <em>The Astronomical Journal</em>, 135, 2008
              <p>We use computer vision techniques to identify and remove diffraction spikes and reflection halos in the USNO-B Catalog.</p>
              <p>In use at <a href="http://www.astrometry.net">Astrometry.net</a></p>
            </td>
          </tr>
		
        </tbody></table> -->

				
<!--         <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Misc</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td align="center" style="padding:20px;width:25%;vertical-align:middle">
							<heading>Basically <br> Blog Posts</heading>
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
              <br>
              <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain Functions</a>
              <br>
              <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
            </td>
          </tr> -->
					
					
			<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
				<tr>
					<td style="padding:20px;width:100%;vertical-align:middle">
						<br>
			      			<p style="text-align:right;font-size:small;">
							This website was updated on August 9th, 2024.
							<br>
				      			My website uses <a href="https://github.com/jonbarron/website">this template</a>.
			      			</p>
					</td>
				</tr>
				</tbody>
			</table>
		</tbody>
	</table>
	
	</td>
	</tr>
</table>
</body>
</html>
